<h2 id="ceph-commands">Ceph Commands:</h2>
<h3 id="view-status">View Status:</h3>
<pre><code>$ ceph -s
  cluster:
    id:     uuid-x-x-x-x
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum ceph:10.20.30.40:16789
    mgr: 1f2c207d5ec9(active)
    osd: 3 osds: 3 up, 3 in

  data:
    pools:   2 pools, 200 pgs
    objects: 16  objects, 21 MiB
    usage:   3.0 GiB used, 27 GiB / 30 GiB avail
    pgs:     200 active+clean</code></pre>
<h3 id="pools">Pools:</h3>
<p>List pools:</p>
<pre><code>$ ceph osd lspools
1 default
2 volumes</code></pre>
<p>List objects in pool:</p>
<pre><code>$ rados -p volumes ls
rbd_header.10516b8b4567
journal_data.2.10516b8b4567.1
journal_data.2.10516b8b4567.2</code></pre>
<p>View disk space of a pool:</p>
<pre><code>$ rados df -p volumes
POOL_NAME   USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS     RD WR_OPS     WR
volumes   21 MiB      16      0     48                  0       0        0    665 11 MiB    794 16 MiB

total_objects    16
total_used       3.0 GiB
total_avail      27 GiB
total_space      30 GiB</code></pre>
<h2 id="get-pgnum">Get PGNum:</h2>
<pre><code>$ ceph osd pool get volumes pg_num
pg_num: 100</code></pre>
<h3 id="set-dashboard-usernamepassword">Set Dashboard Username/Password:</h3>
<pre><code>ceph dashboard set-login-credentials</code></pre>
<h2 id="ceph-docker-volumes">Ceph Docker Volumes</h2>
<h3 id="dockerized-ceph">Dockerized Ceph:</h3>
<ul>
<li>https://github.com/flaviostutz/ceph-osd</li>
</ul>
<h3 id="docker-volume-plugin">Docker Volume Plugin:</h3>
<ul>
<li>https://github.com/flaviostutz/cepher</li>
</ul>
<h2 id="resources">Resources:</h2>
<ul>
<li>http://docs.ceph.com/docs/mimic/mgr/dashboard/</li>
<li>https://wiki.nix-pro.com/view/Ceph_FAQ/Tweaks/Howtos</li>
</ul>
